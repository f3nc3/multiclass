{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Multiclass Classification\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "x = np.array([0.8, 0.6, 0.7])\n",
        "y = np.array([0, 1, 0])\n",
        "w1 = np.array([[0.2, 0.4, 0.1],\n",
        "              [0.5, 0.3, 0.2],\n",
        "              [0.3, 0.7, 0.8]])\n",
        "w2 = np.array([[0.6, 0.4, 0.5],\n",
        "              [0.1, 0.2, 0.3],\n",
        "              [0.3, 0.7, 0.2]])\n",
        "b1 = np.array([0.1, 0.2, 0.3])\n",
        "b2 = np.array([0.1, 0.2, 0.3])\n",
        "print(f\"\\nOld weights (w1):\\n{w1}\")\n",
        "print(f\"\\nOld biases (b1):\\n{b1}\")\n",
        "print(f\"\\nOld weights (w2):\\n{w2}\")\n",
        "print(f\"\\nold biases (b2):\\n{b2}\")\n",
        "\n",
        "# alpha\n",
        "learning_rate = 0.01\n",
        "epochs = 10000\n",
        "\n",
        "def relu(x):\n",
        "  return np.maximum(0,x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "  return 1*(x>0)\n",
        "\n",
        "def softmax(A):\n",
        "    exps = np.exp(A - np.max(A, axis=-1, keepdims=True))\n",
        "    return exps / np.sum(exps, axis=-1, keepdims=True)\n",
        "\n",
        "# Loss = summation(y_true * log(y_pred))\n",
        "def lossFunction(y_true,y_pred):\n",
        "  return -np.sum(y_true*np.log(y_pred))\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  # Forward propagation\n",
        "\n",
        "  # h1 = w1(x)+b1\n",
        "  h1 = np.dot(x, w1)+b1\n",
        "  # a1 = sig(h1)\n",
        "  a1 = relu(h1)\n",
        "\n",
        "  # h2 = w2(a1)+b2\n",
        "  h2 = np.dot(a1, w2)+b2\n",
        "  # a2 = softmax(h2)\n",
        "  a2 = softmax(h2)\n",
        "\n",
        "  # L = (h2-y)^2\n",
        "  loss = lossFunction(y, a2)\n",
        "  #error = a2 - y\n",
        "  #Error = (a2 - y)\n",
        "  error = a2 - y\n",
        "\n",
        "print(error)\n",
        "print(a2)\n",
        "print(loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sv5AwjqtZ3m",
        "outputId": "e36e3ecc-c664-4c4b-aab4-dda765a92b4f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Old weights (w1):\n",
            "[[0.2 0.4 0.1]\n",
            " [0.5 0.3 0.2]\n",
            " [0.3 0.7 0.8]]\n",
            "\n",
            "Old biases (b1):\n",
            "[0.1 0.2 0.3]\n",
            "\n",
            "Old weights (w2):\n",
            "[[0.6 0.4 0.5]\n",
            " [0.1 0.2 0.3]\n",
            " [0.3 0.7 0.2]]\n",
            "\n",
            "old biases (b2):\n",
            "[0.1 0.2 0.3]\n",
            "[ 0.25502746 -0.58413061  0.32910315]\n",
            "[0.25502746 0.41586939 0.32910315]\n",
            "0.8773840448515435\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  # d(L)/d(w2) = Error * a1\n",
        "  d_L_d_w2 = error.dot(a1.T)\n",
        "\n",
        "  # d(L)/d(b2) = Error\n",
        "  d_L_d_b2 = np.mean(error, axis=0)\n",
        "\n",
        "  d_L_d_hidden = np.dot(error, w2.T)\n",
        "  d_hidden_d_input = relu_derivative(h1)\n",
        "\n",
        "  # d(L)/d(w1) = Error * w2 * x\n",
        "  d_L_d_w1 = np.dot(x.T, d_hidden_d_input * d_L_d_hidden)\n",
        "\n",
        "  # d(L)/d(b1) = Error * w2\n",
        "  d_L_d_b1 = np.mean(d_hidden_d_input * d_L_d_hidden, axis=0)\n",
        "\n",
        "\n",
        "  # Update weights\n",
        "  w2 -= learning_rate * d_L_d_w2\n",
        "  b2 -= learning_rate * d_L_d_b2\n",
        "\n",
        "  w1 -= learning_rate *d_L_d_w1\n",
        "  b1 -= learning_rate * d_L_d_b1\n",
        "\n",
        "  if (epoch + 1) % 1000 == 0:\n",
        "    print(f\"Epoch:{epoch + 1}, Loss: {loss:.4f}\")\n",
        "\n",
        "\n",
        "# Print final outputs\n",
        "print(\"\\nFinal outputs (a2):\\n\", a2)\n",
        "print(\"Final Loss:\\n\", loss)\n",
        "\n",
        "results = []\n",
        "# for input_pair in inputs:\n",
        "#           hidden_input = np.dot(input_pair, weights_input_hidden)+bias_hidden\n",
        "#           hidden_output = sigmoid(hidden_input)\n",
        "#           final_input = np.dot(hidden_output, weights_hidden_output)+bias_output\n",
        "#           final_output = sigmoid(final_input)\n",
        "#           results.append((input_pair,np.round(final_output[0],2)))\n",
        "# print(results)\n",
        "\n",
        "print(f\"\\nUpdated weights (w1):\\n{w1}\")\n",
        "print(f\"\\nUpdated biases (b1):\\n{b1}\")\n",
        "print(f\"\\nUpdated weights (w2):\\n{w2}\")\n",
        "print(f\"\\nUpdated biases (b2):\\n{b2}\")\n",
        "print(f\"\\nOutput (a2) after epoch {epoch + 1}:\\n{a2}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WttBnY3TvtYQ",
        "outputId": "abb02f24-6ee5-45ad-c1f9-e0e1b87db9fe"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch:10000, Loss: 0.8774\n",
            "\n",
            "Final outputs (a2):\n",
            " [0.25502746 0.41586939 0.32910315]\n",
            "Final Loss:\n",
            " 0.8773840448515435\n",
            "\n",
            "Updated weights (w1):\n",
            "[[0.20115017 0.40115017 0.10115017]\n",
            " [0.50115017 0.30115017 0.20115017]\n",
            " [0.30115017 0.70115017 0.80115017]]\n",
            "\n",
            "Updated biases (b1):\n",
            "[0.10058413 0.20058413 0.30058413]\n",
            "\n",
            "Updated weights (w2):\n",
            "[[0.60149895 0.40149895 0.50149895]\n",
            " [0.10149895 0.20149895 0.30149895]\n",
            " [0.30149895 0.70149895 0.20149895]]\n",
            "\n",
            "Updated biases (b2):\n",
            "[0.1 0.2 0.3]\n",
            "\n",
            "Output (a2) after epoch 10000:\n",
            "[0.25502746 0.41586939 0.32910315]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gxGKQdVWvtzj"
      },
      "execution_count": 12,
      "outputs": []
    }
  ]
}